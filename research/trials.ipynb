{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a017b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c51ddcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\GIT\\\\policy-bot\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8950931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e9ea253",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"D:\\GIT\\policy-bot\")  # Adjust based on actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b14238f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\GIT\\\\policy-bot'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bca6379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8851235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc330f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extracted_data=load_pdf_file(data='Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b272708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92951857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a043011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 124\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f53bbe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08d11628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f56e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "711ce74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (0.30.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from huggingface_hub) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade huggingface_hub sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94b02f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bened\\AppData\\Local\\Temp\\ipykernel_17628\\2661704553.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
      "c:\\Users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "576e68ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66b3ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bfb72b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06fd1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cad9336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"policybot\",\n",
       "    \"metric\": \"cosine\",\n",
       "    \"host\": \"policybot-msts9pb.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 384,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"policybot\"\n",
    "\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\", \n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "#OPENAI_API_KEY = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e10fdb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc3775d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a290393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x1c62e7e50c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "008756ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "501aa1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is LIC Jeevan Utsav?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61f7660b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='6758129e-50c3-4096-8dec-a944f2490b4f', metadata={'creationdate': '2025-03-27T12:23:08+05:30', 'creator': 'Adobe InDesign 20.2 (Windows)', 'moddate': '2025-03-27T12:24:38+05:30', 'page': 1.0, 'page_label': '4', 'producer': 'Adobe PDF Library 17.0', 'source': 'Data\\\\102268- Jeevan Utsav Sales Brochure.pdf', 'total_pages': 32.0, 'trapped': '/False'}, page_content='2\\nLIC’s Jeevan Utsav (UIN: 512N363V02)\\n(A Non-Par, Non-Linked, Individual, \\nSavings, Whole Life Insurance Plan)\\nLIC’s Jeevan Utsav is a Non-Par, Non-Linked, Individual, \\nSavings, Whole Life Insurance plan. This plan provides \\nfinancial support to family in case of unfortunate death of Life \\nAssured and survival benefits in the form of Regular Income \\nBenefit or Flexi Income Benefit as per the option chosen for \\nsurviving policyholder.'),\n",
       " Document(id='5a7b0663-2cb7-4576-b3c7-beb3019b9c50', metadata={'creationdate': '2025-03-27T12:23:08+05:30', 'creator': 'Adobe InDesign 20.2 (Windows)', 'moddate': '2025-03-27T12:24:38+05:30', 'page': 1.0, 'page_label': '4', 'producer': 'Adobe PDF Library 17.0', 'source': 'Data\\\\102268- Jeevan Utsav Sales Brochure.pdf', 'total_pages': 32.0, 'trapped': '/False'}, page_content='LIC’s Jeevan Utsav is a non-par product under which benefits \\npayable on death or survival are guaranteed and fixed \\nirrespective of actual experience. Hence the policy is not \\nentitled to discretionary benefits like bonus etc. or share in \\nSurplus.\\nThis Plan can be purchased Offline through Licensed agents, \\nCorporate agents, Brokers, Insurance Marketing Firms as well \\nas Online directly through website www.licindia.in. However,'),\n",
       " Document(id='585427d1-b715-44db-84d2-ddcd2acc04d9', metadata={'creationdate': '2025-03-27T12:23:08+05:30', 'creator': 'Adobe InDesign 20.2 (Windows)', 'moddate': '2025-03-27T12:24:38+05:30', 'page': 31.0, 'page_label': '34', 'producer': 'Adobe PDF Library 17.0', 'source': 'Data\\\\102268- Jeevan Utsav Sales Brochure.pdf', 'total_pages': 32.0, 'trapped': '/False'}, page_content='Registered Office:\\nLife Insurance Corporation of India\\nCentral Office, \\nYogakshema, Jeevan Bima Marg, Mumbai – \\n400021.\\nWebsite: www.licindia.in \\nRegistration Number: 512\\n32')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64af4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face API Token: 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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch the Hugging Face API token from environment\n",
    "huggingface_api_token = os.getenv(\"HF_API_TOKEN\")\n",
    "\n",
    "# OPTIONAL: Print to verify (during debugging)\n",
    "print(f\"Hugging Face API Token: {huggingface_api_token}\")\n",
    "\n",
    "# Initialize the HuggingFace model\n",
    "hf_model = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",  \n",
    "    huggingfacehub_api_token=huggingface_api_token,  # <-- Now from env!\n",
    "    model_kwargs={\"temperature\": 0.4, \"max_new_tokens\": 500}\n",
    ")\n",
    "\n",
    "# Define the system prompt\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the provided context to answer the user's question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Keep your response concise, within three sentences.\"\n",
    ")\n",
    "\n",
    "# Set up the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"Context:\\n{context}\\n\\nQuestion:\\n{input}\")\n",
    "])\n",
    "# # Define system prompt\n",
    "# system_prompt = (\n",
    "#     \"You are an assistant for question-answering tasks. \"\n",
    "#     \"Use the following pieces of retrieved context to answer \"\n",
    "#     \"the question. If you don't know the answer, say that you \"\n",
    "#     \"don't know. Use three sentences maximum and keep the \"\n",
    "#     \"answer concise.\"\n",
    "#     \"\\n\\n\"\n",
    "#     \"{context}\"\n",
    "# )\n",
    "\n",
    "# # Set up the prompt template\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_prompt),\n",
    "#         (\"human\", \"{input}\"),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77e9ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(hf_model, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a91b79f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an assistant for question-answering tasks. Use the provided context to answer the user's question. If you don't know the answer, say you don't know. Keep your response concise, within three sentences.\n",
      "Human: Context:\n",
      "2\n",
      "LIC’s Jeevan Utsav (UIN: 512N363V02)\n",
      "(A Non-Par, Non-Linked, Individual, \n",
      "Savings, Whole Life Insurance Plan)\n",
      "LIC’s Jeevan Utsav is a Non-Par, Non-Linked, Individual, \n",
      "Savings, Whole Life Insurance plan. This plan provides \n",
      "financial support to family in case of unfortunate death of Life \n",
      "Assured and survival benefits in the form of Regular Income \n",
      "Benefit or Flexi Income Benefit as per the option chosen for \n",
      "surviving policyholder.\n",
      "\n",
      "LIC’s Jeevan Utsav is a non-par product under which benefits \n",
      "payable on death or survival are guaranteed and fixed \n",
      "irrespective of actual experience. Hence the policy is not \n",
      "entitled to discretionary benefits like bonus etc. or share in \n",
      "Surplus.\n",
      "This Plan can be purchased Offline through Licensed agents, \n",
      "Corporate agents, Brokers, Insurance Marketing Firms as well \n",
      "as Online directly through website www.licindia.in. However,\n",
      "\n",
      "Registered Office:\n",
      "Life Insurance Corporation of India\n",
      "Central Office, \n",
      "Yogakshema, Jeevan Bima Marg, Mumbai – \n",
      "400021.\n",
      "Website: www.licindia.in \n",
      "Registration Number: 512\n",
      "32\n",
      "\n",
      "Question:\n",
      "What is LIC Jeevan Utsav?\n",
      "Answer:\n",
      "LIC Jeevan Utsav is a Non-Par, Non-Linked, Individual, Savings, Whole Life Insurance plan that provides financial support to the family in case of the unfortunate death of the Life Assured and survival benefits in the form of Regular Income Benefit or Flexi Income Benefit. It's a non-par product, meaning benefits payable on death or survival are guaranteed and fixed, and it's not entitled to discretionary benefits or a share in Surplus.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What is LIC Jeevan Utsav?\"})\n",
    "\n",
    "# Extract only the part after \"Assistant:\"\n",
    "full_output = response[\"answer\"]\n",
    "match = re.search(r\"Assistant:\\s*(.+)\", full_output, re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    final_answer = match.group(1).strip()\n",
    "else:\n",
    "    final_answer = full_output.strip()\n",
    "\n",
    "print(final_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "243a28de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an assistant for question-answering tasks. Use the provided context to answer the user's question. If you don't know the answer, say you don't know. Keep your response concise, within three sentences.\n",
      "Human: Context:\n",
      "2\n",
      "LIC’s Jeevan Utsav (UIN: 512N363V02)\n",
      "(A Non-Par, Non-Linked, Individual, \n",
      "Savings, Whole Life Insurance Plan)\n",
      "LIC’s Jeevan Utsav is a Non-Par, Non-Linked, Individual, \n",
      "Savings, Whole Life Insurance plan. This plan provides \n",
      "financial support to family in case of unfortunate death of Life \n",
      "Assured and survival benefits in the form of Regular Income \n",
      "Benefit or Flexi Income Benefit as per the option chosen for \n",
      "surviving policyholder.\n",
      "\n",
      "LIC’s Jeevan Utsav is a non-par product under which benefits \n",
      "payable on death or survival are guaranteed and fixed \n",
      "irrespective of actual experience. Hence the policy is not \n",
      "entitled to discretionary benefits like bonus etc. or share in \n",
      "Surplus.\n",
      "This Plan can be purchased Offline through Licensed agents, \n",
      "Corporate agents, Brokers, Insurance Marketing Firms as well \n",
      "as Online directly through website www.licindia.in. However,\n",
      "\n",
      "Registered Office:\n",
      "Life Insurance Corporation of India\n",
      "Central Office, \n",
      "Yogakshema, Jeevan Bima Marg, Mumbai – \n",
      "400021.\n",
      "Website: www.licindia.in \n",
      "Registration Number: 512\n",
      "32\n",
      "\n",
      "Question:\n",
      "What is LIC Jeevan Utsav?\n",
      "Answer:\n",
      "LIC Jeevan Utsav is a Non-Par, Non-Linked, Individual, Savings, Whole Life Insurance plan that provides financial support to the family in case of the unfortunate death of the Life Assured and survival benefits in the form of Regular Income Benefit or Flexi Income Benefit. It's a non-par product, meaning benefits payable on death or survival are guaranteed and fixed, and it's not entitled to discretionary benefits or a share in Surplus.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is LIC Jeevan Utsav?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24347488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an assistant for question-answering tasks. Use the provided context to answer the user's question. If you don't know the answer, say you don't know. Keep your response concise, within three sentences.\n",
      "Human: Context:\n",
      "3 2.00% 10 9.22% 17 16.44% 24 23.66% 31 30.88%\n",
      "4 3.03% 11 10.25% 18 17.47% 25 24.69% 32 31.91%\n",
      "5 4.06% 12 11.28% 19 18.50% 26 25.72% 33 32.94%\n",
      "6 5.09% 13 12.31% 20 19.53% 27 26.75% 34 33.97%\n",
      "7 6.13% 14 13.34% 21 20.56% 28 27.78% 35 & \n",
      "above\n",
      "35.00%\n",
      "\n",
      "57 1042200 480000 100000 0 1480000 0 1336752\n",
      "58 1042200 480000 100000 0 1480000 0 1336752\n",
      "59 1042200 480000 100000 0 1480000 0 1336752\n",
      "60 1042200 480000 100000 0 1480000 0 1336752\n",
      "61 1042200 480000 100000 0 1480000 0 1336752\n",
      "62 1042200 480000 100000 0 1480000 0 1336752\n",
      "63 1042200 480000 100000 0 1480000 0 1336752\n",
      "64 1042200 480000 100000 0 1480000 0 1336752\n",
      "65 1042200 480000 100000 0 1480000 0 1480000\n",
      " Note:\n",
      " 1. This illustration is showing the flow of benefits up to\n",
      "\n",
      "Benefit Summary:\n",
      "Policy \n",
      "Year\n",
      "(End \n",
      "of \n",
      "Year)\n",
      "Annual-\n",
      "ized Pre-\n",
      "mium1 \n",
      "(Cumula-\n",
      "tive)\n",
      "Guar-\n",
      "anteed \n",
      "Addi-\n",
      "tion\n",
      "Guaranteed Benefits (in Rs.)\n",
      "Non  \n",
      "Guar-\n",
      "anteed \n",
      "Benefits  \n",
      "(in Rs.)\n",
      "Flexi \n",
      "In-\n",
      "come \n",
      "Ben-\n",
      "efit2\n",
      "Ma-\n",
      "turity \n",
      "Ben-\n",
      "efit\n",
      "Death \n",
      "Benefit\n",
      "Mini-\n",
      "mum \n",
      "Guar-\n",
      "anteed \n",
      "Sur-\n",
      "render \n",
      "Value3\n",
      "Special \n",
      "Sur-\n",
      "render \n",
      "Value\n",
      "1 86850 40000 0 0 1040000 0 9398\n",
      "2 173700 80000 0 0 1080000 52110 19931\n",
      "3 260550 120000 0 0 1120000 93593 89613\n",
      "4 347400 160000 0 0 1160000 178548 144791\n",
      "\n",
      "Question:\n",
      "What is stats?\n",
      "\n",
      "Assistant: I'm sorry for any confusion, but the context you've provided doesn't contain any information that could be interpreted as \"stats\". It seems to be a table showing premiums, guaranteed and non-guaranteed benefits, flexi income benefit, maturity benefit, death benefit, and minimum guaranteed surrender value for a policy over four years. If you're asking for a statistical analysis of this data, I would need to know what specific information you're interested in.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is stats?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14b6b6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bened\\anaconda3\\envs\\policybot\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an assistant for question-answering tasks. Use the provided context to answer the user's question. If you don't know the answer, say you don't know. Keep your response concise, within three sentences.\n",
      "Human: Context:\n",
      "3\n",
      "ii. Minimum and \n",
      "Maximum Age at \n",
      "Entry\n",
      "  Premium \n",
      "Paying \n",
      "Term\n",
      "  Minimum \n",
      "Age at Entry \n",
      "(Completed)\n",
      "  Maximum  Age at \n",
      "Entry\n",
      "(Nearer Birthday)\n",
      "5 8 years 65 years\n",
      "6 8 years 65 years\n",
      "7 8 years 65 years\n",
      "8 8 years 65 years\n",
      "9 7 years 65 years\n",
      "10 6 years 65 years\n",
      "11 5 years 64 years\n",
      "12 4 years 63 years\n",
      "13 3 years 62 years\n",
      "14 2 years 61 years\n",
      "15 1 years 60 years\n",
      "16 30 days 59 years\n",
      "iii. Maximum \n",
      "Premium ceasing \n",
      "age\n",
      "75 Years (Nearer Birthday)\n",
      "iv. Minimum Age at \n",
      "the beginning \n",
      "of Policy Year\n",
      "\n",
      "4\n",
      "will commence immediately from the date of acceptance \n",
      "of the risk i.e. from the Date of issuance of policy.  \n",
      "Date of vesting under the plan:  If the policy is issued \n",
      "on the life of a minor, the policy shall automatically vest \n",
      "in the Life Assured on the policy anniversary coinciding \n",
      "with or immediately following the completion of 18 \n",
      "years of age and shall on such vesting be deemed to be a \n",
      "contract between the Corporation and the Life Assured.\n",
      "3. BENEFITS:\n",
      "\n",
      "anniversary on which age nearest birthday of the Life \n",
      "Assured is 70 years, whichever is earlier. If this rider \n",
      "is opted for, in case of accidental death, the Accident \n",
      "Benefit Sum Assured will be payable in lumpsum along \n",
      "with the death benefit under the base plan. Under the \n",
      "policy on the life of minors, this rider will be available \n",
      "from the policy anniversary following completion of \n",
      "age 18 years on receipt of specific request.\n",
      " c)  LIC’s New Term Assurance Rider (UIN: 512B210V02):\n",
      "\n",
      "Question:\n",
      "What is the maturity age in Jeevan Umang?\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is the maturity age in Jeevan Umang?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policybot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
